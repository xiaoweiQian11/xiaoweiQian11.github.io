<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>平凡的世界——现实主义中的理想赞歌</title>
      <link href="/2023/03/27/%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C/"/>
      <url>/2023/03/27/%E5%B9%B3%E5%87%A1%E7%9A%84%E4%B8%96%E7%95%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="关系图">关系图</h1><p>由于担心像百年孤独一样乱成麻花，边看边记了两笔</p><p><img src="https://s2.loli.net/2023/04/09/8CDZRIKTNfLqHgb.png" /></p><h1 id="概要">概要</h1><blockquote><p>同样是看的时候随手记的</p></blockquote><h2 id="第一部">第一部</h2><ul><li>王满银劳教；少安与润叶</li><li>打顾养民；少安被批、相亲</li><li>初识田晓霞；干旱、偷水</li><li>抄诗，救侯玉英；革委会路线矛盾 (周总理逝世）</li><li>金波参军；少安结婚</li><li>毛主席逝世，四人帮被捕、文革结束</li><li>润叶结婚；少安与秀英矛盾</li><li>少平毕业回村教书；润叶与向前不和</li><li>拦截哭咽河、搬迁金家</li><li>金家捉奸斗殴；秀英生子</li></ul><h2 id="第二部">第二部</h2><ul><li>乔伯年上任省委、田福军赴任专员</li><li>政治风气大变、包产到户推行</li><li>少安砖厂起步、少平外出务工</li><li>少安秀莲分家 、少平黄原落户</li><li>少平与晓霞、金波与郝红梅感情线</li><li>向前车祸截肢、田润叶回归婚姻</li></ul><h2 id="第三部">第三部</h2><ul><li>少平前往煤炭厂、金波接手邮政司机；晓霞省报记者、兰香北方工大天体物理，金秀省医学院</li><li>少平晓霞再会面，王世才意外去世</li><li>少安砖厂扩建不善终倒闭</li><li>润生与红梅、金强与孙卫红感情线</li><li>杜丽丽出轨、黄原赴京“讨饭”</li><li>南部大洪水田晓霞救人溺亡；少平悲痛欲绝独自赴两年之约</li><li>少安砖厂回归正轨、少平矿难毁容</li><li>少安做善事建学校 、秀莲操劳多年患肺癌</li><li>少平回到煤矿走向未知的明天</li></ul><h1 id="随笔">随笔</h1><p>​ 给人的第一感觉像是一面镜子</p>]]></content>
      
      
      <categories>
          
          <category> 读书笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 小说 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes for &#39;Deep Learning on Graphs&#39;</title>
      <link href="/2022/11/07/Deep_Learning_on_Graphs/"/>
      <url>/2022/11/07/Deep_Learning_on_Graphs/</url>
      
        <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2023/04/09/hIB6cQbd7jyLDRA.png" /></p><h1 id="introduction">Introduction</h1><h2 id="feature-learning-on-graphs-a-brief-history">1.5 Feature Learning on Graphs: A Brief History</h2><h3 id="feature-selection-on-graphs">Feature Selection on Graphs</h3><p>Traditional feature selection assumes that data instances are independence and identically distributed.</p><p>However, data samples in many applications are embedded in graphs that are inherently not i.i.d.</p><h3 id="representation-learning-on-graphs">Representation Learning on Graphs</h3><ul><li><strong>Spatial approaches</strong> explicitly leverage the graph structure, such as spatially close neighbors.</li><li><strong>Spectral approaches</strong> utilize the spectral view of graphs by taking advantage of graph Fourier transform and the inverse graph Fourier transform.</li></ul><p>In the era of DL, GNNs have been rapidly developed in the following aspects:</p><p>(Write a separate blog based this paragraph)</p><h1 id="foundations-of-graphs">Foundations of Graphs</h1><h2 id="graph-representaion">2.2 Graph Representaion</h2><p>Graph</p><p>Adjacent Matrix</p><h2 id="properties-and-measures">2.3 Properties and Measures</h2><h3 id="degree">Degree</h3><p>Neighbors</p><h3 id="connectivity">Connectivity</h3><h3 id="centrality">Centrality</h3><h4 id="degree-centrality">Degree Centrality</h4><p>​ <span class="math inline">\(c_d(v_i)=d(v_i)\)</span></p><h4 id="eigenvector-centrality">Eigenvector Centrality</h4><p><span class="math display">\[\begin{align}&amp; c_e(v_i)=\frac{1}{\lambda}\sum^{N}_{j=1}A_{i,j}\cdot c_{e}(v_j)  \\&amp; c_e = \frac{1}{\lambda}A \cdot {c_e}\end{align}\]</span></p><p><em>where <span class="math inline">\(c_e \in \mathbb{R}^{N}\)</span> is a vector containing the centrality scores of all nodes in the graph.</em></p><blockquote><p><strong>Citation</strong>: A real squared matrix with positive elements has a unique largest eigenvalue and its corresponding eigenvector has all positive elements.</p></blockquote><p>​ We choose <span class="math inline">\(\lambda\)</span> as the largest eigenvalue and its corresponding eigenvector as the centrality score vector.</p><h4 id="katz-centrality">Katz Centrality</h4><h4 id="between-centrality">Between Centrality</h4><h2 id="spectral-gprah-theory">2.4 Spectral Gprah Theory</h2><h3 id="laplacian-matrix">Laplacian Matrix</h3><p><span class="math display">\[\begin{align}&amp;(Laplacian \space Matrix) \\&amp;L=D-A \\&amp;(Normalized \space Laplacian \space Matrix)\\&amp;L=D^{-1/2}(D-A)D^{-1/2}=I-D^{-1/2}AD^{-1/2}\end{align}\]</span></p><p><em>where D is a diagonal degree matrix <span class="math inline">\(D=diag(d(v_i),...,d(v_{|\mathcal{V}|}))\)</span></em></p><p>​ Laplacian Matrix is a positive semi-definite.</p><h3 id="the-eigenvalues-and-eigenvectors-of-the-laplacian-matrix">The Eigenvalues and Eigenvectors of the Laplacian Matrix</h3><p><strong>Theorem</strong>: The eigenvalues of graph's Laplacian matrix L are nonnegtive.</p><p><strong>Theorem</strong>: Given a graph <span class="math inline">\(\mathcal{G}\)</span>, the number of 0 eigenvalues of its Laplacian Matrix L equals the number of connected component in the graph.</p><p>​ For a graph with N nodes, there are N eigenvalues.</p><blockquote><p><strong>Citation</strong>: 一个n阶矩阵一定有n个特征值（包括重根，也可能是复根）一个n阶实对称矩阵一定有n个实特征值（包括重根）每个特征值至少有一个特诊向量，不同特征值对应的特征向量线性无关。</p></blockquote><h2 id="graph-signal-processing">2.5 Graph Signal Processing</h2><p>​ A graph signal consists of a graph and a mapping function <span class="math inline">\(f\)</span> defined on node domain, which maps the nodes to real values: <span class="math display">\[f: \mathcal V \rightarrow \mathbb R^d,\]</span> where d is the dimension of the vector associated with each node. <strong>Denote mapped values for all nodes as f with f[i] corresponding to node <span class="math inline">\(v_i\)</span>.</strong></p><p>​ A graph is smooth if the values in connected nodes are similar. A smooth graph signal is low frequency, because the values change slowly across the graph via the edges.</p><p>​ The value <span class="math inline">\(\mathbf{f^{T}Lf}\)</span> is called the smoothness(or frequency) of the signal <strong>f</strong>. <span class="math display">\[\mathbf{f^{T}Lf}=\frac{1}{2}\sum_{v_i\in\mathcal{V}}\sum_{v_j\in\mathcal{N}(v_i)}(\mathbf f[i]-\mathbf f[j])^2\]</span> ​ The Laplacian quadratic form can measure the smoothness(or the frequency) of a graph signal because it's the summation of the square of the difference between all pairs of connected nodes. When the signal is smooth, the smoothness is small.</p><p>​ Similar to classical signal can be denoted in time domain and frequency domain, graph signal can also be representation in spatial domain, and spectral domain(or frequency domain).</p><h3 id="graph-fourier-transform">Graph Fourier Transform</h3><h1 id="graph-neural-network">Graph Neural Network</h1><h2 id="introduction-1">5.1 Introduction</h2><h2 id="the-graph-gnn-framework">5.2 The Graph GNN Framework</h2><h2 id="graph-filters">5.3 Graph Filters</h2><h3 id="spectral-based-graph-filters">5.3.1 Spectral-Based Graph Filters</h3><h4 id="graph-spectral-filter">Graph Spectral Filter</h4><p><img src="https://s2.loli.net/2023/04/09/7xAfXQGOisU9DCZ.png" /></p><h4 id="spectral-based-graph-filter">Spectral-Based Graph Filter</h4><p>Highlights of Poly-Filter:</p><ol type="1"><li>The Poly-Filter is spatially localized. Because it only involves K-hops when calculating output for a specific node;</li><li>Eigendecomposition isn't needed;</li></ol><p>Disadvantage of Poly-Filter:</p><ol type="1"><li><p>The bias of the polymial (i.e., 1, x, x^2, ...) is not an orthogonal bias. Hence the coeffient of the biases are dependent on each other. An update in one coeffient may lead to changes in other coeffients.</p><blockquote><p>正交多项式，指一族多项式中任意两个多项式在某个区间内内积积分为0；</p></blockquote></li></ol><h4 id="chebyshev-polynomial-and-cheby-filter">Chebyshev Polynomial and Cheby-Filter</h4><p>为了解决基底正交的问题，引入正交多项式中的切比雪夫多项式（同样是正交多项式的还有勒让德多项式）</p><p><img src="https://s2.loli.net/2023/04/09/CmPlfk4XJS39LrK.png" /></p><h4 id="gcn-filter-simplified-cheby-filter-involving-1-hop-neighbors">GCN-Filter: Simplified Cheby-Filter Involving 1-Hop Neighbors</h4><h4 id="graph-filter-for-multichannel-graph-signal">Graph Filter for Multichannel Graph Signal</h4><h3 id="spatial-based-graph-filters">5.3.2 Spatial-Based Graph Filters</h3><h4 id="graphsage-filter">GraphSAGE Filter</h4><h4 id="gat-filter">GAT-Filter</h4><h4 id="ec-filter">EC-Filter</h4><h4 id="ggnn-filter">GGNN-Filter</h4><h4 id="mo-filter">Mo-Filter</h4><h4 id="mpnn-a-general-framework-for-spatial-based-graph-filters">MPNN: A General Framework for Spatial-Based Graph Filters</h4><h2 id="graph-pooling">5.4 Graph Pooling</h2><h2 id="parameter-learning-for-graph-neural-network">5.5 Parameter Learning for Graph Neural Network</h2>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Operating Systems Review Notes</title>
      <link href="/2022/09/02/Operating_Systems_Review_Notes/"/>
      <url>/2022/09/02/Operating_Systems_Review_Notes/</url>
      
        <content type="html"><![CDATA[<h1 id="在线预览效果">在线预览效果</h1><div class="row">    <embed src="https://xiaoweiQian11.github.io/pdf/Operating_Systems_Review_Notes.pdf" width="100%" height="550" type="application/pdf"></div><h1 id="使用pdf.js实现在线pdf阅读">使用PDF.js实现在线PDF阅读</h1><p><a href="https://xiaoweiqian11.github.io/pdfjs/web/viewer.html?file=../data/Operating_Systems_Review_Notes.pdf">Operating_Systems_Review_Notes.pdf (xiaoweiqian11.github.io)</a></p>]]></content>
      
      
      <categories>
          
          <category> PDF </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>林轩田机器学习基石</title>
      <link href="/2021/10/28/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/"/>
      <url>/2021/10/28/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/</url>
      
        <content type="html"><![CDATA[<h1 id="林轩田机器学习基石">林轩田机器学习基石</h1><h2 id="大纲"><strong>大纲</strong></h2><p><strong>When Can Machine Learn?</strong></p><ul><li>The Learning Problem</li><li>Learning to Answer Yes/No</li><li>Types of Learning</li><li>Feasibility of Learning</li></ul><p><strong>Why Can Machine Learn?</strong></p><ul><li>Training versus Testing</li><li>Theory of Generalization</li><li>The VC Dimension</li><li>Nosie and Error</li></ul><p><strong>How Can Machine Learn?</strong></p><ul><li></li></ul><h2 id="一the-learning-problem">一、The Learning Problem</h2><p>1.<strong>介绍</strong>了什么是机器学习</p><p>2.机器学习在生活各个方面的<strong>应用</strong></p><p>3.Formalize the learning problem:</p><p>​ input, output,</p><p>​ target function(<strong>unknown</strong> pattern to be learned),</p><p>​ data(training examples),</p><p>​ hypothesis(learned formula to be used)</p><p><img src="https://z3.ax1x.com/2021/11/23/opxQK0.md.jpg" /></p><p>4.机器与其他领域</p><p><strong>Machine Learning</strong>: use data to compute hypothesis <strong><em>g</em></strong> that approximates taget <strong><em>f</em></strong></p><p><strong>Data Mining</strong>: use huge data to find property that is interesting</p><p><strong>Artificial Intelligence</strong>: compute someting that shows intelligent behavior</p><p><strong>Statistics</strong>: use data to make inference about an unknown process</p><h2 id="二learning-to-answer-yesno二元分类">二、Learning to Answer Yes/No（二元分类）</h2><h3 id="perceptron-hypothesis-set"><strong>Perceptron Hypothesis Set</strong></h3><p>提出将感知机用于二分类</p><p><img src="https://z3.ax1x.com/2021/11/23/opxJ54.jpg" /></p><p>单层感知机，在二维情况下又称为<strong>线性分类器</strong>。</p><p>​ 二维指特征向量只有二维，再加上一个bias，hypothesis函数就相当于二维坐标系中一条直线，将二维空间二分类。</p><p><img src="https://z3.ax1x.com/2021/11/23/opxyIe.md.jpg" /></p><h3 id="perceptron-learning-algorithm-pla"><strong>Perceptron Learning Algorithm (PLA)</strong></h3><p>​ 目标函数f是未知的，算法要让hypothesis(g)逼近那个理想的 f。首先要在训练集D中使g ≈ f。首先给出一个初始g，再开始逐步修正。t代表当前训练的批次，n代表当前是哪个点在代入计算（？概念模糊）。</p><p>​ 如果label与计算结果相反，w 加上yx 以此向理想情况逼近。</p><p><img src="https://z3.ax1x.com/2021/11/23/opx2RA.md.jpg" /></p><p>Cyclic PLA：最常见的PLA编写方式。从1号到n号点，一个个尝试，没有错误就到下一个点，有错误就进行修正</p><p><img src="https://z3.ax1x.com/2021/11/23/opxbGj.md.jpg" /></p><p>课程视频中的可视化很好</p><h3 id="guarantee-of-pla"><strong>Guarantee of PLA</strong></h3><p>linear separability (线性可分)。只有满足线性可分的数据集才可以用PLA分类，否则算法一直进行无法停下。</p><p>如果线性可分，那么又能否确保PLA最终会停下呢？（wf是目标权重）</p><ul><li>线性可分意味着下面第一个不等式。而由它可以推出wt与ideal wf 的内积越来越大。但是如果向量模越来越大也会导致内积变大，所以暂时不能说明两个向量越来越接近</li></ul><p><img src="https://z3.ax1x.com/2021/11/23/opzFzR.md.png" /></p><ul><li>||wt||的增长不会很快（y是±1）。并且可以证明随着T的增加，两个向量夹角余弦值越来越大，即夹角越来越小。所以两个向量越来越接近。</li></ul><p><img src="https://z3.ax1x.com/2021/11/23/opzWY4.md.jpg" /></p><p><img src="https://z3.ax1x.com/2021/11/23/opzoOx.md.jpg" /></p><p>证明过程</p><p><img src="https://z3.ax1x.com/2021/11/23/opzbTO.md.jpg" alt="opzbTO.md.jpg" style="zoom: 67%;" /></p><p>​ 综上可以说明PLA会停下来。</p><p><strong>PLA的总结与反思</strong></p><p><img src="https://z3.ax1x.com/2021/11/23/o9SSXt.md.jpg" /></p><p>我们拿到数据时并不知道是不是线性可分也就是不知道PLA是否会停下，就算知道它是线性可分的也不知道他多久会停</p><h3 id="non-separable-data"><strong>Non-Separable data</strong></h3><p>即使原本线性可分的数据，如果加入了noise也不再线性可分。noise通常是很少的，所以大体上仍然存在理想中的f。</p><p>如果这时继续采用不断修正w的方法，那么要从所有w里选择一个犯错误次数最少的。但目前尚无好的方法解决。</p><p><img src="https://z3.ax1x.com/2021/11/23/o9SmXq.md.jpg" /></p><p>那么这时该怎么办呢？</p><p>一种用于找到好的g的PLA的改进算法（当有noise，非线性可分时）</p><p><img src="https://z3.ax1x.com/2021/11/23/o9S1NF.md.jpg" /></p><p>pocket相对于PLA更慢</p><p><img src="https://z3.ax1x.com/2021/11/23/o9SG9J.jpg" /></p><h2 id="三type-of-learning">三、Type of Learning</h2><h4 id="learning-with-different-output-space">Learning with Different Output Space</h4><p>根据输出空间的不同，学习问题可以分为多种类型</p><p>二分类：输出空间只有两个值</p><p>多元分类：输出是一个有限的离散空间</p><p>回归分析：输出空间是一个实数集</p><p>结构学习：输出是一个结构。输入一个句子，输出词性</p><h4 id="learning-with-different-data-label">Learning with Different Data Label</h4><p>根据数据标签的情况不同可以将学习问题分类</p><p>supervised（监督学习）：分类、回归、标注</p><p>unsupervised（无监督学习）：clustering（聚类）、density estimation（密度估计）、outlier detection（异常值检测）</p><p>semi-supervised（半监督学习）：face identifier、药效预测</p><p>reinforcement learning(强化学习）：所有数据有标注，但不一定正确，我们对输出结果给予正或负反馈</p><h4 id="learning-with-different-protocol">Learning with Different Protocol</h4><p>根据与机器沟通方式不同将学习问题分类</p><p>batch：将数据一次性投给机器，得出一个最好的学习结果</p><p>online：一次一次将数据投给系统，每一次得到更好的学习结果</p><p>active: improve hypothesis with fewer labels(hopefully) by asking questions strategically。当取得label很困难时</p><h4 id="learning-with-different-input-space">Learning with Different Input Space</h4><p>concrete feature：很直观的具有物理意义的特征</p><p>raw feature：不那么直观，需要一些转换的特征</p><p>abstract feature：非常抽象的特征，需要进行多次转换</p><h2 id="四feasibility-of-learning">四、Feasibility of Learning</h2><p>when can machines?</p><p><strong>learning is PAC-possible, if enough statistical data and finite |H|</strong></p><hr /><h3 id="learning-is-impossible">Learning is Impossible？</h3><p>​ 机器学习真的可行吗。用不同解释方式，会得到不同规律不同结果。但机器学习目的是模型能预测训练集之外的正确结果，而不是在已知的数据集D寻求最佳效果。</p><p>​ 一个数学化的例子：有8种输入，训练样本D有5个。根据训练样本对应的输出y，假设有8个hypothesis，这8个hypothesis在D上，对5个训练样本的分类效果效果都完全正确。但在另外3个测试数据上，不同的hypothesis表现有好有坏。</p><p><img src="https://z3.ax1x.com/2021/11/23/o9pp8J.md.jpg" /></p><p>​ 也即，在已知数据D上 g ≈ f，但是在D以外的未知数据上，g ≈ f 不一定成立。<strong>对应到现实情况就是数据量太少，模型复杂度太高，出现了过拟合。</strong></p><p><em>想要在D以外的数据中更接近目标函数似乎是做不到的，只能保证对D有很好的分类结果</em>。这是机器学习中的NFL定理（No Free Lunch)。它说明除非加上一些假设，否则无法保证g能在D以外的数据集上一定能分类或预测正确。</p><hr /><h3 id="probability-to-the-rescue">Probability to the Rescue</h3><p>解决可行性问题的可能方法</p><p>​ 如果有一个装有很多（不可数）橙色球和绿色球的罐子，我们能不能推断橙色球的比例u？统计学上的做法是，从罐子中随机取出N个球，作为样本，计算这N个球中橙色球的比例v，那么就估计出罐子中橙色球的比例约为v。但v是否就一定等于u呢？</p><p>由第一节的思考我们很容易知道--不一定。但是从统计上来说，v非常接近那个未知的u。</p><p>补充知识，霍夫丁不等式设x1,......,xn是取值在[0,1]上的==独立==随机变量，我们定义这些变量的经验均值为： <span class="math display">\[\overline{X} = \frac{1}{n}(X_1+\cdots+X_n)\]</span> Hoeffding定理1的不等式为： <span class="math display">\[P(\overline{X}-E[\overline{X}])\leq e^{-2nt^2}\]</span> Hoeffgding定理2是上述不等式的泛化形式。当xi的取值严格限制在[ai,bi] <span class="math display">\[P(\overline{X}-E(\overline{X})\geq t)\leq exp(-\frac{2n^2t^2}{\sum_{i=1}^{n}(b_i-a_i)^2})\\P(|\overline{X}-E(\overline{X})|\geq t)\leq 2exp(-\frac{2n^2t^2}{\sum_{i=1}^{n}(b_i-a_i)^2})\]</span> 回到机器学习问题，当N很大、x满足独立分布，v和u差距会被限制在 ϵ 之内。"v = u"被称为PAC。</p><p><img src="https://z3.ax1x.com/2021/11/23/o9pmPe.md.jpg" /></p><p>上述理论可行的原因是：</p><ul><li>对于任意 N 和 ϵ 都是有效的</li><li>不等式右侧并不依赖于那个我们不知道的u，是可以被确定的</li><li>更大的N或更宽松的 ϵ 都能提高v ≈ u 的可能性</li></ul><hr /><h3 id="connection-to-learning">Connection to Learning</h3><p>​ 将上述理论<strong>映射</strong>到学习问题中</p><ul><li><p>未知的橙球的比例 相当于 hypothesis等于目标函数 f 的可能性</p></li><li><p>球属于罐子 相当于 x属于机器学习样本空间X</p></li><li><p>橙色球 相当于 h 与 f 不相等</p></li><li><p>绿色球 相当于 h 与 f 相等</p></li><li><p>从罐子里拿出的N个球 相当于 在样本数据集D上验证 h （i.i.d. 独立同分布）</p></li></ul><p>​ 所以，如果样本够大，就能从已知的h ≠ f 的概率推得未知数据上h ≠ f 的可能性，前后两个概率相等是PAC的。</p><p>​ 从局部推得全局是机器学习能工作的本质</p><p><img src="https://z3.ax1x.com/2021/11/23/o9plrt.md.jpg" /></p><p>​ P是(distribution)一种分布方式，训练集和测试集中的所有<span class="math inline">\(x_n\)</span>（样本）都是独立同分布，这样便满足了Hoeffding不等式的条件，于是，训练集中预测错误的几率可以推广到训练集之外h ≠ f的几率。（实际上机器学习并不总是要求数据独立同分布，不少问题中要求样本采用自同一个分布是希望训练数据集训练得到的模型可以合理用于数据集，而同分布的假设使得这样的作法行得通）</p><p>​ h是hypothesis set中一个取出即固定的函数（我们的目的是求他们中最好的那个 g）</p><p>​ 这里引入两个值，Eout、Ein。Eout代表对于所有依P分布的x，h ≠ f 的概率；Ein表示对于训练集样本h ≠ y 的概率。（[[A]]：A为正，则整体取1；反之。取0）</p><p><img src="https://z3.ax1x.com/2021/11/23/o9pgG4.md.jpg" /></p><p>同样根据霍夫丁不等式，in-sample error 和 out-of-sample error两者也是PAC的</p><p><img src="https://z3.ax1x.com/2021/11/23/o9pLzd.md.jpg" /></p><p>​ 综上，一个数据集在一个h上 <span class="math inline">\(E_{in} = E_{out}\)</span> 是PAC的。我们的理想情况是Ein小，那么Eout也小，可以得到 g≈f，但如果Ein本身就比较大，那么就不能判断g≈f。所以如果我们只有一个h，利用数据做出的判断是不准确的。所以我们接下来的证明是演算法A能够从H中选取最好的一个h。</p><hr /><h3 id="connection-to-real-learning">Connection to Real Learning</h3><p>将情况拓展到有多个hypothesis</p><p>​ 还是老问题，有一个h在n个数据集中的某个数据集上预测全对，能否说明这个h对于这个D就很好？不一定。霍夫丁不等式仅在行维度上保证：n个数据集在一个h上时，BAD data出现的概率很小。但D数量一多，就很有可能会出现BAD data了。PS：在D上Ein与Eout差距很大（称为bad sample），我们就将这个D视为对于h的BAD data。</p><p><img src="https://z3.ax1x.com/2021/11/23/o9pzeP.md.jpg" /></p><p>​ 现在我们有多个h，A要在这些h中做选择。一个好的资料D是无论怎么选择h都不会有BAD情况出现，只要这个D使一个h产生了BAD情况，那么这个D就是BAD data，使我们的演算法不能正常做选择。</p><p><img src="https://z3.ax1x.com/2021/11/23/o99VLq.md.jpg" /></p><p>然而根据union bound（布尔不等式）有如下结果：</p><p><img src="https://z3.ax1x.com/2021/11/23/o99c0P.md.jpg" /></p><p>​ 即，当|H| = M 且为有限个、且N足够大时，无论怎么选择h，Ein与Eout都是PAC的。自然地，A就要去选择Ein最小的那个h。而A选择出来的最小的Ein(g)≈0，由于PAC，对应的Eout也≈0</p><p>​ 综上所述，机器学习在添加了上述假设条件后是可能的，我们向机器学习的可行性迈进了一步。</p><p>那么如果hypothesis set中有无限多个h（如perceptron）又该怎么证明呢？接下来几节将说明。</p><hr /><h2 id="五training-versus-testing">五、Training versus Testing</h2><h3 id="recap-and-review">Recap and Review</h3><p>下面是基于统计学的机器学习流程图：</p><p><img src="https://z3.ax1x.com/2021/11/23/o9CunI.png" /></p><p>​ （训练样本和测试h的样本都来自同一个数据分布(i.i.d)作为机器学习的前提。）</p><p>复习一下前四章：</p><ul><li>第一讲介绍了机器学习的定义，目的是找出最好的g≈f，保证Eout≈0</li><li>第二讲介绍了如何让Ein≈0，即使用PLA、pocket等演算法</li><li>第三讲介绍了机器学习问题的分类</li><li>第四讲介绍了机器学习的可行性，证明了在一些假设条件下Ein≈Eout</li></ul><p>综上，在前四讲中我们将learning可行性的核心拆分成两个问题：</p><ul><li>能确定<span class="math inline">\(E_{in}\)</span>和<span class="math inline">\(E_{out}\)</span>足够接近吗？（接下来证明）</li><li>能确定<span class="math inline">\(E_{in}\)</span>已经足够小了吗？（与很多因素有关，可用演算法选择较小的）</li></ul><p><img src="https://z3.ax1x.com/2021/11/23/o9C0EV.md.jpg" /></p><p>​ 那么我们要讨论的 M 又在这两个问题中产生怎样的影响呢？</p><p>M很小：</p><ul><li>对于问题1有利，根据公式可知P[BAD]会更小，Ein与Eout更加接近</li><li>对于问题2不利，可选择的h太少了，Ein可能还不够小</li></ul><p>M很大：</p><ul><li>对于问题1不利；对于问题2有利</li></ul><p>​ 那是不是就说M无限大时根本就不可能呢？接下来3讲将解决这个问题。</p><hr /><h3 id="effective-number-of-lines">Effective Number of Lines</h3><p>为什么hypothesis的数量M趋于无穷大时会出问题呢</p><p>​ 将不同的BAD event看作集合，实际上是有重叠部分的（例如两条错误分类的直线，他们可能相差非常小完全可以被视为一类），但推导过程中使用了union bound，就将不等式右边的范围扩大了(over-estimating)</p><p>那么我们就想找出重叠的部分，将hypothesis划分为有限类</p><p>​ 以直线划分平面内的点为例，如果单从点的分类来看。一个点时有两类线；两个点时有四类线；三个点时最多有八类线，如果三点共线只有六类线；四个点时就只有14类线了。 发现有效直线的数量一直小于等于2^N是有限的，也就是说无限数量的直线可以被分为有限个种类。 我们希望：存在一个effective(N)能替换掉无限大的M，并且远小于2^N（远小于的话就避免了和指数函数相乘不好判断结果到底是否有限）。那么不等式右边在M趋近于无穷大、且N很大时仍趋近于0，也就进一步证明了机器学习的可行性。</p><p><img src="https://z3.ax1x.com/2021/11/23/o9C54O.md.jpg" /></p><p>​ 接下来我们的工作就在于证明有这样一个effective(N)存在。</p><hr /><h3 id="effective-number-of-hypothesis">Effective Number of Hypothesis</h3><p>​ 引入一个概念：二分类(dichotomy)，将空间中的点用直线分为正负两类。</p><ul><li>hypothesis H是平面上所有直线的集合，可能是无限的；</li><li>dichotomy H是平面上能将所有点分类的直线的种类，上界是<span class="math inline">\(2^N\)</span>；</li></ul><p>​ 作为一个有界量，dichotomy能不能直接取代M呢？这是不行的。因为dichotomy H是一个依赖于input的集合，这种对输入的过度依赖不利于之后的证明，所以再引入一个概念。</p><p>​ 引入成长函数(growth function)。成长函数在某个n的各种可能取值中取最大的一个(如：当有3个点时，可分直线种类可能有6、8两种取值，而我们的成长函数取最大)。这个函数和dichotomy一样是以<span class="math inline">\(2^N\)</span>为上界。</p><p>那么又如何计算成长函数呢？</p><p>​ 对于一维的positive rays：</p><p><img src="https://z3.ax1x.com/2021/11/23/o9KEZD.md.jpg" /></p><p>​ 对于一维的positive interval：</p><p><img src="https://z3.ax1x.com/2021/11/23/o9Kmid.jpg" alt="o9Kmid.jpg" style="zoom:80%;" /></p><p><img src="https://z3.ax1x.com/2021/11/23/o9KGdg.jpg" /></p><p>​ (为什么 <span class="math inline">\({N+1\choose 2}\)</span> 展开后是这样的多项式形式？我不理解)</p><p>​ 对于二维的convex sets：对于一个凸区域，点在区域内取1、区域外取-1。（如果n个点的所有分类情况都能被hypothesis set包含，那么将这种情形称为shattered）显然shattered时，成长函数为<span class="math inline">\(2^N\)</span>。</p><p><img src="https://z3.ax1x.com/2021/11/23/o9Ktij.md.jpg" /></p><hr /><h3 id="break-point">Break Point</h3><p>我们由上一讲已经知道了几种不同的成长函数：</p><p><img src="https://z3.ax1x.com/2021/11/23/o9KcFJ.md.jpg" /></p><p>如果成长函数要取代M <span class="math display">\[P[|E_{in}(g)-E_{out}(g)|&lt;\varepsilon]\leq 2\cdot m_H(N)\cdot exp(-2\varepsilon^2N)\]</span> ​ 如果m是polynomial的，那么当N很大时可以保证右边趋于0。如positive rays和positive intervals</p><p>​ 如果m是exponential的，两个指数函数相乘，并不能保证右边趋近于0。如convex sets</p><p>而对于二维perceptrons，它的成长函数是怎样的呢？</p><p>​ 首先引入break piont的概念。将不能shattered by H的k号点称为该H的break point(即<span class="math inline">\(m_H(k)&lt;2^k\)</span>)。并且第一个break point之后的所有点都是break point。</p><p><img src="https://z3.ax1x.com/2021/11/23/o9Kzm8.jpg" /></p><p>​ 接下来我们有了一个猜测：当没有break point时，成长函数是2的N次方；当break point出现后，成长函数的复杂度是N的k-1次方，也就是我们的多项式形式。</p><p><img src="https://z3.ax1x.com/2021/11/23/o9Mkpn.md.jpg" /></p><p>​ 如果能证明这个猜想，就说明成长函数取代M能使不等式右边趋近于0，从而证明机器学习的可行性。</p><hr /><h2 id="六theory-of-generalization">六、Theory of Generalization</h2><h3 id="restriction-of-break-point">Restriction of Break Point</h3><p>​ 从上一讲我们知道第一个break point之后的点都是break point，那么除此之外，break point的出现有没有为我们之后的点带来更强的限制呢？</p><p>​ 一个例子：假设最小的break point k=2。当N=1，m = 2；当N=2，m最大为3（要小于<span class="math inline">\(2^2\)</span>）；当N=3，要求任意两个点不能shattered（shatter的意思是对N个点，能够分解为<span class="math inline">\(2^N\)</span>种dichotomies）,画图分析可知，m最大为4。</p><p><img src="https://z3.ax1x.com/2021/11/23/o9MgHS.md.jpg" /></p><p>​ 我们发现break point之后的m受到了很大的限制，会远小于<span class="math inline">\(2^N\)</span>。总结一下，m的大小受两方面影响，一是N的大小，一是 k的大小。如果我们证明在给定N和k情况下，m最大值的上界是polynomial的，就能用成长函数代替M</p><p><img src="https://z3.ax1x.com/2021/11/23/o9QSjx.jpg" /></p><hr /><h3 id="bounding-function-basic-cases">Bounding Function-Basic Cases</h3><p>上一节我们想要找出一个成长函数的上界，现在我们正式定义一个边界函数(bounding function)，B(N, k)</p><p>​ 将bounding function定义为当break point为k时，成长函数<span class="math inline">\(m_H(N)\)</span>的上界。定义了边界函数的好处在于不需要考虑H的细节（H是将平面上点用直线分开的所有hypothesis h的集合），不管是什么形式的hypothesis，只要N和k确定，就可以确定边界函数，简化了证明问题的复杂度。</p><p><img src="https://z3.ax1x.com/2021/11/23/o9Q1Ug.jpg" /></p><p>​ 接下来证明这个边界函数是否以多项式速度增长</p><p>​ 我们以填表的形式来探索边界函数的一般规律：</p><ul><li>当k = 1，B(N, 1)恒为1</li><li>当N &lt; k，易得<span class="math inline">\(B(N, k)=2^N\)</span></li><li>当N = k，此时N个点第一次出现不能被shattered的情况，故边界函数<span class="math inline">\(B(N, k) = 2^N-1\)</span></li></ul><p><img src="https://z3.ax1x.com/2021/11/23/o9QD54.md.jpg" /></p><p>下一节将分析N &gt; k的情况如何求解</p><hr /><h3 id="bounding-function-inductive">Bounding Function-Inductive</h3><p>​ 我们先从B(4,3)的情况入手。遍历所有情况后列出了符合k=3的11种情形在左图中。我们发现可以将他们进行分类：橙色的四对前三个点都相同，只改变x4；紫色的三组前三个都不相同。</p><p><img src="https://z3.ax1x.com/2021/11/23/o9QHxI.md.jpg" /></p><p>​ 这样分类后，我们将橙色组去掉x4并去重后的向量数设为<span class="math inline">\(\alpha\)</span>，将紫色组的向量数设为<span class="math inline">\(\beta\)</span>，那么就会有 <span class="math display">\[B(4,3)=11=2\alpha+\beta\]</span> ​ 又因为k=3，所以前三个点也是不能被shattered的，所以会有 <span class="math display">\[\alpha+\beta \leq B(3,3)\]</span> <img src="https://z3.ax1x.com/2021/11/23/o9QjZ8.png" /></p><p>​ 另一方面，在橙色组不能被三个点shattered，而x4又是成对出现的，所以前三个点的组合甚至不能被两个点shattered，这样进一步加强条件又会得到 <span class="math display">\[\alpha \leq B(3,2)\]</span> ​ 所以有 <span class="math display">\[B(4,3)=2\alpha +\beta \leq B(3,3)+B(2,2) \]</span> ​ 更一般地 <span class="math display">\[B(N,k)\leq B(N-1, k)+B(N-1, k-1)\]</span> ​ 根据上述不等式关系可以继续填补之前的table</p><p><img src="https://z3.ax1x.com/2021/11/23/o9lEZT.png" /></p><p>​ 由上述公式加以推导可以得到B(N,k)更一般的关系式： <span class="math display">\[B(N,k)\leq \underbrace{\sum_{i=0}^{k-1}{N\choose{i}}}_{highest\:term\:N^{k-1}}\]</span> 证明过程：</p><p><img src="https://z3.ax1x.com/2021/11/23/o9l4O0.jpg" alt="o9l4O0.jpg" style="zoom:80%;" /></p><p>（但为什么最高次项是N的k-1次方我还是不理解，并且这个式子实际上是等式，日后再来解决）</p><p><img src="https://z3.ax1x.com/2021/11/23/o9lXlR.png" /></p><p>​ 给出了bounding function的上界之后也就解决了我们要将成长函数限制在多项式复杂度之内的目标。更广泛地说，对于任意一个模型，我们只要能找到第一个break point k（有限），那么就能说它的成长函数有界且上界polynomial。</p><h3 id="a-pictorial-proof">A Pictorial Proof</h3><p>​ 虽然break point存在可以说明成长函数有多项式上界，但实际上不是简单地将Hoeffding中的M替换成<span class="math inline">\(m_H(N)\)</span>就可以说。替换M需要with a few changes:</p><ul><li>replace Eout by Ein'</li><li>decompose H by kind</li><li>use Hoeffding without replacement</li></ul><p><img src="https://z3.ax1x.com/2021/11/23/o9lzm6.jpg" /></p><p>(推导过程实在看不懂，将来看懂了再来补充)</p><p>​ 对于2D perceptrons，它的break point是4，那么成长函数<span class="math inline">\(m_H(N)=O(N^3)\)</span>。所以，我们可以说2D perceptrons是可以进行机器学习的，只要找到hypothesis能让Ein≈0，就能保证Ein≈Eout。</p><p>​ Anyway，我们经过引入break point、成长函数、bounding function一连串概念并推导，知道了M是可以被上界为多项式复杂度的成长函数给替代的，再根据Hoeffding，推导出了VC bound，最终证明了只要break point存在，机器学习就是可行的。但本章仅证明了二维的perceptron是可行的（仅说明了二维perceptron的break point存在）。更广泛的结论之后再说。</p><hr /><h2 id="七the-vc-dimension">七、The VC Dimension</h2><p>本讲介绍了VC Dimension的主要概念，同时说明了VC Dimension与Ein、Eout的关系</p><h3 id="definition-of-vc-dimension">Definition of VC Dimension</h3><p>​ 上一讲我们推出了成长函数的上界bounding function的上界<span class="math inline">\(N^{k-1}\)</span>，那么我们可以得出一个更松弛的界限：</p><p><img src="https://z3.ax1x.com/2021/11/23/o91kpd.jpg" /></p><p>​ 结合上一讲证明的不等式，可以得到更多关于VC Bound的结论：</p><p><img src="https://z3.ax1x.com/2021/11/23/o91Mtg.md.jpg" /></p><p>将成长函数换成我们新的上界之后，坏事发生的概率只由N和k两个变量决定，而一般情况下认为样本N足够大，所以发生坏事的概率只与k值有关了。第一个break point的k值确定有限，那么这是一个好的hypothesis set；样本N足够大，那么这是一个好的数据集D；能够选择一个较小的Ein，那么这是一个好的演算法A。集合了这三点，那么我们可以说理想情况下机器可以学习了。</p><p>​ 因为现在坏事发生概率是由k决定的，所以我们要仔细研究break point。下面给出VC Dimension的概念（break point的正式名称），将可以shatter的N的最大值定为<span class="math inline">\(d_{VC}(H)\)</span>。</p><p><img src="https://z3.ax1x.com/2021/11/23/o91GXq.md.jpg" /></p><p>当<span class="math inline">\(N\leq d_{VC}\)</span>时，H可以被N个inputs shattered</p><p>若<span class="math inline">\(k &gt; d_{VC}\)</span>，k是H的break point</p><p>​ 常见例子的VC Dimension：</p><p><img src="https://z3.ax1x.com/2021/11/23/o91B9J.png" /></p><p>​ 之前仅与k有关的坏事发生的概率现在变为仅与VC维有关。有一个确定的、有限的 <span class="math inline">\(d_{VC}\)</span>就是一个好的H，一个hypothesis set的<span class="math inline">\(d_{VC}\)</span>确定了，就能说明Ein ≈ Eout。并且这与演算法A、训练or测试样本的distribution P、理想中的目标函数f都没有关系。</p><p>​ 至此我们证明了在最坏状况下也能保证机器学习可行性的泛化。（？）</p><p>PS：单一笔N个输入的资料不能被H shattered，不能说明这是break point。因为成长函数是dichotomy的最大值，当所有N个inputs 的资料都不能被shattered时，才能说这是break point。</p><hr /><h3 id="vc-dimension-of-perceptrons">VC Dimension of perceptrons</h3><p>​ 我们已经给出了一维和二维感知机的VC维，现在向更高维度进发。根据之前的规律，我们有了如下猜想：</p><p><img src="https://z3.ax1x.com/2021/11/23/o91sj1.png" /></p><p>​ 证明<strong>d维</strong>感知机的猜想，可以证 <span class="math inline">\(\leq 和 \geq\)</span> 同时成立。</p><p>证明<span class="math inline">\(d_{VC}\geq d+1\)</span>:</p><p>​ 只需证明<strong>存在</strong>d+1条inputs的输入可以被H shattered，就说明VC维应该大于等于d+1。</p><p>​ 给出一个输入X，有d+1条inputs，每个inputs有d+1维（原来是d维的，但加上了第一列bias）。易得这个X可逆（我也不知道具体为什么可逆，以后再来推导）。</p><p>​ 如果存在这样一个输入可以被shattered，那么对于任何一种y的组合，都可以找到一个 w，使得Xw = y。由于X可逆，容易证得这个结论。</p><p><img src="https://z3.ax1x.com/2021/11/23/o93Svn.png" /></p><p>证明<span class="math inline">\(d_{VC}\leq d+1\)</span>：</p><p>​ 需证明<strong>所有</strong>d+2条inputs的输入都不能被H shattered，才能说明VC维小于等于d+2。</p><p>​ 给出一个输出X，有d+2条inputs，每个inputs有d+1维。由于向量组数大于向量维数，该向量组线性相关，所以可以写出下列线性表出形式。假设线性表出前面的系数的正负性与wx相同，可知等式左边的正负性是受限的，所以就不能被shattered。这个结论对任意一个d+2个inputs的输入都成立。</p><p><img src="https://z3.ax1x.com/2021/11/23/o93FET.md.png" /></p><p>综上，<span class="math inline">\(d_{VC}= d+1\)</span>成立。</p><hr /><h3 id="physical-intuition-of-vc-dimension">Physical Intuition of VC Dimension</h3><p>这一节介绍VC维的物理直觉</p><p>​ hypothesis的参数向量w中参数的个数是w的自由度；H的中hypothesis的数量M也可以类比为假设集的自由度；再次类比，<span class="math inline">\(d_{VC}\)</span>代表了hypothesis分类能力的强弱，是有效的二元分类的自由度。</p><p><img src="https://z3.ax1x.com/2021/11/23/o93uK1.md.png" /></p><p><img src="https://z3.ax1x.com/2021/11/23/o93dqP.md.jpg" /></p><p>​ 结合一维、二维perceptron中<span class="math inline">\(d_{VC}\)</span>和parameters的关系，可以有一个physical intuition：</p><p><img src="https://z3.ax1x.com/2021/11/23/o93sPg.md.png" /></p><p>这个约等式不总是对的，但在实际应用中，如果想要确定VC维，看看自由度通常都是好的方式。</p><p>​ 结合第五讲中M对机器学习两个核心问题的影响，现在可以给出VC维对两个核心问题的影响：</p><p><img src="https://z3.ax1x.com/2021/11/23/o93Rrq.md.jpg" /></p><ul><li>VC维较小。有利于缩小坏事发生的几率；但自由度较小，限制了模型的分类能力</li><li>VC维较大。扩大了坏事发生的几率；但自由度较大，提升了模型的分类能力</li></ul><p>这其中蕴含的philosophical message需要细细体会。</p><hr /><h3 id="interpreting-vc-dimension">Interpreting VC Dimension</h3><p>这一节将更加深入地讨论VC维在不同层面的意义（更多在哲学方面体会）</p><p>​ 首先是VC维从模型复杂度的理解。设不等式右边即坏事发生的几率为<span class="math inline">\(\delta\)</span>，那么好事发生的几率为1-<span class="math inline">\(\delta\)</span>。 将等式重新推导后可以得到<span class="math inline">\(\epsilon\)</span>的表达式（<span class="math inline">\(\epsilon\)</span>代表了g的<strong>泛化能力</strong>；其值越小，泛化能力越强）</p><p><img src="https://z3.ax1x.com/2021/11/23/o935IU.md.jpg" /></p><p>​ 进一步推导可以得到Eout的信赖区间。一般来说我们更看重信赖区间的右端，因为右端决定了未来预测误差的上界。而最后一项被称为模型复杂度的罚项(penalty for model complexity)，它与样本复杂程度N、假设空间的<span class="math inline">\(d_{VC}\)</span>、<span class="math inline">\(\epsilon\)</span>相关。上述内容类似于《统计学习方法》中的泛化误差上界。</p><p><img src="https://z3.ax1x.com/2021/11/23/o93TG4.md.jpg" /></p><p>​ 根据罚项可以得到一点VC维传达出的message。下面给出Ein、Eout、model complexity与<span class="math inline">\(d_{VC}\)</span>的关系图，从中我们可 以得知：为了最小的Eout，不能单纯为了减小Ein而增加<span class="math inline">\(\Omega\)</span>，两者是共同作用在Eout上的。最好是选择中间某个<span class="math inline">\(d_{VC}\)</span>（对应到model中即为feature个数）。</p><p><img src="https://i.loli.net/2021/11/24/uZFYLywnR16JbkN.png" alt="20170505143707333.jpg"  /></p><p>​ 下面介绍VC Dimension从样本复杂度(sample complexity)的理解。假设现在要求Ein、Eout误差控制在0.1；坏事发生的概率控制在0.1；而现有的H的<span class="math inline">\(d_{VC}\)</span>是3，那么请问数据量N应为多少才能达到要求？</p><p><img src="https://i.loli.net/2021/11/24/HNdtQBoEw1f9aJI.png" alt="20170505145501721.jpg" style="zoom:67%;" /></p><p><img src="https://i.loli.net/2021/11/24/W94Dkg8rLMY3ojt.png" alt="20170505145842865.jpg" style="zoom:67%;" /></p><p>​ 理论上样本复杂度是<span class="math inline">\(d_{VC}\)</span>的10000倍，但实际应用中只需要10倍即可。这中间的差距是由于VC Bound这个上界非常宽松，那么为什么VC Bound会如此宽松呢？究其原因在于我们的推导过程每一步都在loose：Hoeffding不等式对any distribution, any target function均成立；成长函数是dichotomy的最大值，不是dichotomy的真正数量，使我们可以用任何一笔数据，而不是过于依赖手上的数据；我们为了将推广机器学习可行性的模型，用<span class="math inline">\(N^{d_{VC}}\)</span>代替成长函数，使<span class="math inline">\(d_{VC}\)</span>相同的any假设空间都用一个上界；我们为了让演算法A可以自由做选择，使用了union bound。</p><p>​ 虽然VC Bound非常宽松，但想在如此广泛的范围上收紧它却很困难。而且有利的是，它对所有模型的宽松程度基本是一样的，所以不同模型之间还是可以横向比较。</p><p>​ 相比于VC Bound提供理论上的严格限制，<strong>philosophical message of VC bound important for improving ML</strong>。如：我们不应该为了追求更强的分类能力而忽略了复杂度的飞速提升。</p><h3 id="总结">总结</h3><p>​ 本讲主要介绍了VC Dimension就是最大的non-break point。然后，我们得到了Perceptrons在d维度下的VC Dimension是d+1。接着，我们在物理意义上，将dvc与自由度联系起来。最终得出结论dvc不能过大也不能过小。选取合适的值，才能让Eout足够小，使假设空间H具有良好的泛化能力。</p><hr /><h2 id="八noise-and-error">八、Noise and Error</h2><h3 id="noise-and-probabilistic-target">Noise and Probabilistic Target</h3><p>​ 没noise时，x确定了，y就确定为f(x)；有noise时，某些x对应的y有可能不是f(x)的（f是target function），我们可以将这种noise视为 y 按 P(y|x) 分布。我们可以不加证明地说，有noise时，只要x独立同分布于P(x)，y独立同分布于P(y|x)，或者说数据集(x, y)独立同分布于f(x, y)，之前有关VC Dimension的证明仍然是正确的，仍可以判断Ein ≈ Eout。</p><p><img src="https://i.loli.net/2021/11/24/kWjE5LPcYzVQDH7.jpg"  /></p><p>​ P(y|x)称为target function：characterizes behavior of 'mini-target' on one x</p><p>​ 为什么称为target function？需要我们对它的意义进行理解：</p><ul><li>我们可以视其为理想中的、很ideal的target function和noise的叠加产生的结果</li><li>无noise情况下的deterministic target function也可视为一种特殊的target function</li><li>在常见的上要做得好（？）（w.r.t. with repect to 关于、谈及)</li></ul><p><img src="https://i.loli.net/2021/11/24/G7lnawdRKQho5Su.jpg" /></p><p>加入了noise之后流程图如下，PLA是典型的有noise情况下的算法。</p><p><img src="https://i.loli.net/2021/11/24/ZSnbgaGQf2O1Jzq.png" alt="20170616080950726.jpg"  /></p><hr /><h3 id="error-measure">Error Measure</h3><p>​ 如何衡量目前选出g的好坏就要与目标函数f进行比较，这个衡量过程就是error measure。常用的衡量办法是pointwise error measure（逐点误差测量）。实际上就是对每个点计算error再取平均：</p><p><img src="https://i.loli.net/2021/11/24/StmywUdEusVYxjB.png" /></p><p>​ 根据所应用的学习问题的不同，error函数也有所不同：</p><ul><li>0-1 error。判断预测是否正确，常用于classification</li><li>squared error。计算预测值与目标值差的平方，常用于regression</li></ul><p>​ 不同的error函数使演算法对目标函数的优化方式不同，error measure在learning的过程中是非常重要的。</p><p><img src="https://i.loli.net/2021/11/24/LWMcYFhBZ2Rf5KP.png" /></p><p>​ 课程中不加证明地说，VC Bound的理论虽然之前在很受限的情况下证明，但其实对于more H和error都是可推广的。</p><hr /><h3 id="algorithmic-error-measure">Algorithmic Error Measure</h3><p>​ 直接进行error measure很多时候是难以做到的（可以参考之前PLA算法中的NP hard），实际应用在算法中时，我们常常将难以进行的error measure 替换成便于算法计算的其他量之间的关系，algorithm error measure <span class="math inline">\(\hat{err}\)</span>。</p><p><img src="https://i.loli.net/2021/11/24/b8lp5BWzgqsLmT1.jpg" /></p><p>​ 所以我们要对流程图再做一些更新，我们想要的确实是这个error，但为了让我们的算法更加和谐，往往会采用<span class="math inline">\(\hat{err}\)</span></p><p><img src="https://i.loli.net/2021/11/24/bUYt1riuRg5C8Ew.png" /></p><hr /><h3 id="weighted-classification">Weighted Classification</h3><p>​ 之前error measure对所有错误都一视同仁，直接求平均，但在实际应用中不是这样的。例如二元分类中，将正类误判为负类（false reject）和 将负类误判为正类（false accept）常常是不能一视同仁的。识别超市会员指纹时，将会员判为非会员显然比将非会员判为会员的错误更为严重，后者可能只是减少一些盈利，前者可能损害口碑；再比如识别读取机密文件人员的指纹，将非权限人员判为有权限人员显然错误更加严重。显然对于不同error的权重应该有一些区分（weighted classification）。</p><p>​ 之前已经不加证明地提出在这些情况下VC Bound依然成立，那么现在只要解决两大核心问题中的让Ein ≈ 0即可。（下图中加了上标w代表有权重的error measure）</p><p><img src="https://i.loli.net/2021/11/24/tF9PQjwvBWqR1gm.jpg" /></p><p>​ 有了weighted classification之后，我们对应的算法也要做出一些改变。以改进pocket算法为例，假设要求false accept的惩罚是false reject的1000倍，我们不需要真的把false accept的数据复制一千份再来计算error，而是采用virtual copy，将随机到这样的点的概率扩大1000倍，其他的仍和之前的pocket算法一样，寻找一个更小的来取代现有的。</p><p>​ 这种泛化的方式可以推广到其他算法</p><p><img src="https://i.loli.net/2021/11/24/EhveYrIsMLCTgm1.png" /></p><p>对于这种不平衡的数据，就需要更合理的weight来进行平衡，不然就会出现这种荒谬的结论。</p><p><img src="https://i.loli.net/2021/11/24/hX6eKvy4fC87g3a.jpg" /></p><h3 id="总结-1">总结</h3><p>本讲的几个要点：</p><ul><li>有noise情况下VC Dimension依然成立</li><li>cost function的计算方法</li><li>对于不同类错误的weight classification</li></ul><hr /><h2 id="九linear-regression">九、Linear Regression</h2><h3 id="linear-regression-problem">Linear Regression Problem</h3><p>​ 线性回归的计算式类似于感知机，但要求输出是实数。目的是找到直线、平面或更高维的超平面（lines/hyperlanes)并使它有最小的残差（residuals），类似于图中的红线。</p><p><img src="https://i.loli.net/2021/11/24/gxvHM8mwOckQ3fT.jpg" /></p><p><img src="https://i.loli.net/2021/11/24/bkHhR1YSVuGlxdA.png" /></p><p>​ 常用的误差计算是平方误差。</p><p><img src="https://i.loli.net/2021/11/24/XG1cU5LoqIYle9d.jpg" /></p><p>​ 这里仍然不加证明的说VC Dimension 的相关结论成立（包括有noise等情况）。</p><h3 id="linear-regression-algorithm">Linear Regression Algorithm</h3><p>​ 机器学习第一个核心问题解决了，接下来就是 how to minimize Ein。</p><p>​ 将Ein做一些变换。首先去掉连加符号，转换为向量模；再将w提取出来，赋予前后两个向量新名字：</p><p><img src="https://z3.ax1x.com/2021/11/22/oSp4Mt.png" /></p><p>​ 括号内的函数是连续的、可微的、凸函数（国外把这种二阶导严格大于零的类似山谷的函数叫做convex functin）。让Ein最小的w，应当使梯度为零。</p><p><img src="https://z3.ax1x.com/2021/11/22/oSp5sP.md.png" /></p><p>​ 不加证明地给出求此向量梯度的公式：</p><p><img src="https://z3.ax1x.com/2021/11/22/oSpIqf.md.png" /></p><p>​ 由VC Dimension的结论，数据量N远大于d（？），这种行数远大于列数向量相乘，很大可能性<span class="math inline">\(X^TX\)</span>是可逆的（？)而对于奇异矩阵（singular matrix），有很多最优化方式，其中一种如下所示（证明上述结论需要"矩阵理论知识）。</p><p><img src="https://z3.ax1x.com/2021/11/22/oSpfxI.md.png" /></p><p>通常建议在coding时直接使用pseudo-inverse。</p><p>​ 总结一下，计算线性回归的流程如下：</p><ul><li>从数据集D中得到input matrix X、output vector y</li><li>计算pseudo-inverse</li><li>得到<span class="math inline">\(w_{lin}\)</span></li></ul><h3 id="generalization-issue">Generalization Issue</h3><p>​ 关于线性回归是不是一种机器学习算法有一些争论：</p><ul><li>NO! 这是analytic solution（closed-form），是瞬时完成的（instantaneous）；并且没有迭代地优化Ein、Eout</li><li>Yes！它最优化 (optimal) 了Ein；VC维是有限的，满足核心问题一；在求pseudo-inverse过程中是迭代的</li></ul><p>​ 我们在此说明：只要最终<span class="math inline">\(E_{out}(w_{LIN})\)</span>是好的，那么learning happened ! 下面介绍一种比VC Dimension简单的方法证明在线性回归问题中Ein ≈ Eout。</p><p>​</p><h3 id="for-binary-classification">for Binary Classification</h3><hr /><h2 id="十logistic-regression">十、Logistic Regression</h2><h3 id="logistic-regression-problem">Logistic Regression Problem</h3><h3 id="logistic-regession-error">Logistic Regession Error</h3><h3 id="gradient-of-logistic-regression-error">Gradient of Logistic Regression Error</h3><h3 id="gradient-descent">Gradient Descent</h3><hr /><h2 id="十一linear-models-for-classification">十一、Linear Models for Classification</h2><h3 id="binary-classification">Binary Classification</h3><h3 id="stochastic-grad.-descent">Stochastic Grad. Descent</h3><h3 id="multiclass-via-logistic">Multiclass via Logistic</h3><h3 id="multiclass-via-binary">Multiclass via Binary</h3><hr /><h2 id="十二nonlinear-transformer">十二、Nonlinear Transformer</h2><h3 id="quadratic-hypothesis">Quadratic Hypothesis</h3><h3 id="nonlinear-transform">Nonlinear Transform</h3><h3 id="price-of-nonlinear-transform">Price of Nonlinear Transform</h3><h3 id="structured-hypothesis-sets">Structured Hypothesis Sets</h3><hr />]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 统计机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
